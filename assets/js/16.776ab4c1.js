(window.webpackJsonp=window.webpackJsonp||[]).push([[16],{367:function(t,a,s){"use strict";s.r(a);var e=s(11),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"machine-translation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#machine-translation"}},[t._v("#")]),t._v(" Machine Translation")]),t._v(" "),s("h2",{attrs:{id:"seq2seq"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#seq2seq"}},[t._v("#")]),t._v(" seq2seq")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/",target:"_blank",rel:"noopener noreferrer"}},[t._v("RNN+Attention"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/47063917",target:"_blank",rel:"noopener noreferrer"}},[t._v("Encoder Decoder Attention"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Code"),s("OutboundLink")],1)])]),t._v(" "),s("h2",{attrs:{id:"bert"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#bert"}},[t._v("#")]),t._v(" Bert")]),t._v(" "),s("p",[t._v("Bert uses exact the same structure as transformers(proposed in attention is all you need), but is pre-trained in a novel way to facilitate bidirectional learning")]),t._v(" "),s("p",[t._v("Besides positional embedding and word embedding, bert proposes a token_type embedding(like segment ids) to extend for other tasks")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"http://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer1"),s("OutboundLink")],1),t._v(" "),s("ul",[s("li",[t._v("multi-head self-attention, relation to other tokens: "),s("eq",[s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("msub",[s("mi",[t._v("q")]),s("mrow",[s("mi",[t._v("t")]),s("mi",[t._v("o")]),s("mi",[t._v("k")]),s("mi",[t._v("e")]),s("mi",[t._v("n")])],1)],1),s("mo",[t._v("∗")]),s("msub",[s("mi",[t._v("k")]),s("mrow",[s("mi",[t._v("o")]),s("mi",[t._v("t")]),s("mi",[t._v("h")]),s("mi",[t._v("e")]),s("mi",[t._v("r")]),s("mi",{attrs:{mathvariant:"normal"}},[t._v("_")]),s("mi",[t._v("t")]),s("mi",[t._v("o")]),s("mi",[t._v("k")]),s("mi",[t._v("e")]),s("mi",[t._v("n")])],1)],1),s("mo",[t._v("=")]),s("mi",[t._v("s")]),s("mi",[t._v("c")]),s("mi",[t._v("o")]),s("mi",[t._v("r")]),s("msub",[s("mi",[t._v("e")]),s("mrow",[s("mi",[t._v("o")]),s("mi",[t._v("t")]),s("mi",[t._v("h")]),s("mi",[t._v("e")]),s("mi",[t._v("r")]),s("mi",{attrs:{mathvariant:"normal"}},[t._v("_")]),s("mi",[t._v("t")]),s("mi",[t._v("o")]),s("mi",[t._v("k")]),s("mi",[t._v("e")]),s("mi",[t._v("n")])],1)],1)],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("q_{token} * k_{other\\_token} = score_{other\\_token}")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"0.6597200000000001em","vertical-align":"-0.19444em"}}),s("span",{staticClass:"mord"},[s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("q")]),s("span",{staticClass:"msupsub"},[s("span",{staticClass:"vlist-t vlist-t2"},[s("span",{staticClass:"vlist-r"},[s("span",{staticClass:"vlist",staticStyle:{height:"0.33610799999999996em"}},[s("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.03588em","margin-right":"0.05em"}},[s("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),s("span",{staticClass:"sizing reset-size6 size3 mtight"},[s("span",{staticClass:"mord mtight"},[s("span",{staticClass:"mord mathdefault mtight"},[t._v("t")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("o")]),s("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("e")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("n")])])])])]),s("span",{staticClass:"vlist-s"},[t._v("​")])]),s("span",{staticClass:"vlist-r"},[s("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[s("span")])])])])]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),s("span",{staticClass:"mbin"},[t._v("∗")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1.06144em","vertical-align":"-0.367em"}}),s("span",{staticClass:"mord"},[s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")]),s("span",{staticClass:"msupsub"},[s("span",{staticClass:"vlist-t vlist-t2"},[s("span",{staticClass:"vlist-r"},[s("span",{staticClass:"vlist",staticStyle:{height:"0.33610799999999996em"}},[s("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"-0.03148em","margin-right":"0.05em"}},[s("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),s("span",{staticClass:"sizing reset-size6 size3 mtight"},[s("span",{staticClass:"mord mtight"},[s("span",{staticClass:"mord mathdefault mtight"},[t._v("o")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("t")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("h")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("e")]),s("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),s("span",{staticClass:"mord mtight",staticStyle:{"margin-right":"0.02778em"}},[t._v("_")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("t")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("o")]),s("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("e")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("n")])])])])]),s("span",{staticClass:"vlist-s"},[t._v("​")])]),s("span",{staticClass:"vlist-r"},[s("span",{staticClass:"vlist",staticStyle:{height:"0.367em"}},[s("span")])])])])]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),s("span",{staticClass:"mrel"},[t._v("=")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"0.79756em","vertical-align":"-0.367em"}}),s("span",{staticClass:"mord mathdefault"},[t._v("s")]),s("span",{staticClass:"mord mathdefault"},[t._v("c")]),s("span",{staticClass:"mord mathdefault"},[t._v("o")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),s("span",{staticClass:"mord"},[s("span",{staticClass:"mord mathdefault"},[t._v("e")]),s("span",{staticClass:"msupsub"},[s("span",{staticClass:"vlist-t vlist-t2"},[s("span",{staticClass:"vlist-r"},[s("span",{staticClass:"vlist",staticStyle:{height:"0.33610799999999996em"}},[s("span",{staticStyle:{top:"-2.5500000000000003em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),s("span",{staticClass:"sizing reset-size6 size3 mtight"},[s("span",{staticClass:"mord mtight"},[s("span",{staticClass:"mord mathdefault mtight"},[t._v("o")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("t")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("h")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("e")]),s("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),s("span",{staticClass:"mord mtight",staticStyle:{"margin-right":"0.02778em"}},[t._v("_")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("t")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("o")]),s("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("e")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("n")])])])])]),s("span",{staticClass:"vlist-s"},[t._v("​")])]),s("span",{staticClass:"vlist-r"},[s("span",{staticClass:"vlist",staticStyle:{height:"0.367em"}},[s("span")])])])])])])])])]),t._v(" ([batch x from_seq x heads x x embedding_head_size] * [batch x to_seq x heads x embedding_head_size] => [batch x heads x from_seq x to_seq])")],1),t._v(" "),s("li",[s("code",[t._v("A [PAD]")]),t._v(" as input, "),s("code",[t._v("[PAD]")]),t._v(" score to "),s("code",[t._v("A")]),t._v(" is 0, but "),s("code",[t._v("A")]),t._v(" to "),s("code",[t._v("[PAD]")]),t._v(" is 0.5. Therefore "),s("code",[t._v("[PAD]")]),t._v(" attends to "),s("code",[t._v("A")]),t._v(" will not update embedding weights for "),s("code",[t._v("[PAD]")]),t._v(", but "),s("code",[t._v("A attends to")]),t._v("[PAD]"),s("code",[t._v("will update embedding weights for")]),t._v("[PAD]`")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://github.com/google-research/bert/issues/58#issuecomment-572287540",target:"_blank",rel:"noopener noreferrer"}},[t._v("positional embedding can be fixed with sinusoidal or a learnable variable"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://kazemnejad.com/blog/transformer_architecture_positional_encoding/",target:"_blank",rel:"noopener noreferrer"}},[t._v("what is sinusoidal"),s("OutboundLink")],1)])])]),t._v(" "),s("li",[s("a",{attrs:{href:"https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer2"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/74516930",target:"_blank",rel:"noopener noreferrer"}},[t._v("Layer Norm"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://datascience.stackexchange.com/questions/49522/what-is-gelu-activation",target:"_blank",rel:"noopener noreferrer"}},[t._v("GeLU"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://nlp.stanford.edu/seminar/details/jdevlin.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pre-training"),s("OutboundLink")],1),t._v(" "),s("ul",[s("li",[t._v("Given "),s("code",[t._v("[MASK]")]),t._v(", module is aware that he needs to predict it")]),t._v(" "),s("li",[t._v("15% of input tokens are masked by 0.8 "),s("code",[t._v("[MASK]")]),t._v(", 0.1 random and different token and 0.1 original token. "),s("a",{attrs:{href:"https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270",target:"_blank",rel:"noopener noreferrer"}},[t._v("Reason"),s("OutboundLink")],1)]),t._v(" "),s("li",[t._v("The output of the "),s("code",[t._v("[CLS]")]),t._v(" token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases)")]),t._v(" "),s("li",[t._v("bert is pre-trained on next-sentence classification, so segment ids is needed for bert pre-training")]),t._v(" "),s("li",[t._v("WordPiece tokenizer, ['hacking'] => ['hack', '##ing']")]),t._v(" "),s("li",[t._v("embedding table => w")]),t._v(" "),s("li",[t._v("segment, position => b\n"),s("ul",[s("li",[t._v("learned position embedding, used by bert")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://kazemnejad.com/blog/transformer_architecture_positional_encoding/",target:"_blank",rel:"noopener noreferrer"}},[t._v("sinusoidal position"),s("OutboundLink")],1),t._v(", this one is fixed but not used by bert")]),t._v(" "),s("li",[t._v("segment ids [0, 0, 1] are extend to [seq_len(3), embedding_size] with random weights, all 0' weights are the same and distinct from 1's")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#history",target:"_blank",rel:"noopener noreferrer"}},[t._v("more info"),s("OutboundLink")],1)])])]),t._v(" "),s("li",[t._v("the embedding process is like a fc layer, if input => one hot encoding (inefficient, thus using embedding lookup)")]),t._v(" "),s("li",[t._v("only attention mask is needed to avoid [PAD] token affecting weight variables being updated in back-propagation\n"),s("img",{attrs:{src:"https://miro.medium.com/max/700/0*m_kXt3uqZH9e7H4w.png",alt:"pre"}})])])]),t._v(" "),s("li",[t._v("Usage\n"),s("img",{attrs:{src:"https://miro.medium.com/max/700/0*eDC9QUkMHJAp-NJg.JPEG",alt:"usage"}}),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/",target:"_blank",rel:"noopener noreferrer"}},[t._v("classification with feature extraction(position and segment are ignored for simple understanding)"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://towardsdatascience.com/bert-to-the-rescue-17671379687f",target:"_blank",rel:"noopener noreferrer"}},[t._v("classification with fine tuning (position and segment are ignored for simple understanding)"),s("OutboundLink")],1)])])]),t._v(" "),s("li",[s("a",{attrs:{href:"https://github.com/google-research/bert/blob/master/modeling.py",target:"_blank",rel:"noopener noreferrer"}},[t._v("Code"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://medium.com/analytics-vidhya/understanding-bert-architecture-3f35a264b187",target:"_blank",rel:"noopener noreferrer"}},[t._v("Parameter calculation 1"),s("OutboundLink")],1),t._v(", "),s("a",{attrs:{href:"https://github.com/google-research/bert/issues/656#issuecomment-554718760",target:"_blank",rel:"noopener noreferrer"}},[t._v("Parameter calculation 2"),s("OutboundLink")],1)])]),t._v(" "),s("h2",{attrs:{id:"word-embedding"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#word-embedding"}},[t._v("#")]),t._v(" Word embedding")]),t._v(" "),s("h3",{attrs:{id:"latent-semantic-analysis"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#latent-semantic-analysis"}},[t._v("#")]),t._v(" latent semantic analysis")]),t._v(" "),s("p",[t._v("SVD， 全局特征的矩阵分解方法")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://moj-analytical-services.github.io/NLP-guidance/LSA.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Reading 1"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://people.eng.unimelb.edu.au/mbouadjenek/papers/wordembed.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Reading 2"),s("OutboundLink")],1)])]),t._v(" "),s("h3",{attrs:{id:"word2vec"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#word2vec"}},[t._v("#")]),t._v(" word2vec")]),t._v(" "),s("p",[t._v("局部上下文")]),t._v(" "),s("p",[t._v("center and context words. skip-ngram: use center to predict context with neural networks")]),t._v(" "),s("ul",[s("li",[t._v("[Reading](https://people.eng.unimelb.edu.au/mbouadjenek/papers/wordembed.pdf")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://ruder.io/word-embeddings-softmax/index.html#negativesampling",target:"_blank",rel:"noopener noreferrer"}},[t._v("Negative sampling"),s("OutboundLink")],1),t._v(" "),s("ul",[s("li",[t._v("softmax on very large context set is time-consuming")]),t._v(" "),s("li",[t._v("noise contrastive estimation converts to binary classification")]),t._v(" "),s("li",[t._v("negative sampling to further simply")])])])]),t._v(" "),s("h3",{attrs:{id:"glove"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#glove"}},[t._v("#")]),t._v(" GloVe")]),t._v(" "),s("p",[t._v("既使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://people.eng.unimelb.edu.au/mbouadjenek/papers/wordembed.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Reading 1"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/42073620",target:"_blank",rel:"noopener noreferrer"}},[t._v("Reading 2"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://github.com/stanfordnlp/GloVe",target:"_blank",rel:"noopener noreferrer"}},[t._v("Back propagation using c"),s("OutboundLink")],1)])]),t._v(" "),s("h3",{attrs:{id:"bert-embedding"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#bert-embedding"}},[t._v("#")]),t._v(" Bert embedding")]),t._v(" "),s("p",[t._v("context aware")]),t._v(" "),s("h2",{attrs:{id:"tokenizer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tokenizer"}},[t._v("#")]),t._v(" Tokenizer")]),t._v(" "),s("h3",{attrs:{id:"byte-pair-encoding"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#byte-pair-encoding"}},[t._v("#")]),t._v(" Byte Pair Encoding")]),t._v(" "),s("p",[t._v("Formed by character n-gram, e.g., "),s("code",[t._v("ana")]),t._v(", "),s("code",[t._v("app")]),t._v(". As in unicode, there are 149k+ characters, hence it requires large number of vocabulary to cover mostly used n-grams. And it is very common to see "),s("code",[t._v("[UNKNOWN]")]),t._v(" in model's input.")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe",target:"_blank",rel:"noopener noreferrer"}},[t._v("Details in huggingface tokenizer summary"),s("OutboundLink")],1)]),t._v(" "),s("h3",{attrs:{id:"byte-level-encoding"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#byte-level-encoding"}},[t._v("#")]),t._v(" Byte Level Encoding")]),t._v(" "),s("p",[t._v("A variation of BPE. Use "),s("strong",[t._v("Byte")]),t._v(" as base unit to form n-gram. Only 256 possible choices for base unit, so it covers mostly used n-grams by only 50k+.\n"),s("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/371300063",target:"_blank",rel:"noopener noreferrer"}},[t._v("聚沙成塔：关于tokenization（词元化）的解疑释惑"),s("OutboundLink")],1)]),t._v(" "),s("h3",{attrs:{id:"wordpiece"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#wordpiece"}},[t._v("#")]),t._v(" WordPiece")]),t._v(" "),s("p",[t._v("Given a training corpus and expected vocabulary size D, find the optimal vocabulary list with size D which produces minimal number of tokens after tokenization with it.")]),t._v(" "),s("h4",{attrs:{id:"bottom-up"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#bottom-up"}},[t._v("#")]),t._v(" Bottom up")]),t._v(" "),s("p",[t._v("Bottom up: described poorly in "),s("a",{attrs:{href:"https://research.google/pubs/pub37842/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Japanese and Korean Voice Search"),s("OutboundLink")],1),t._v(". Similar with "),s("code",[t._v("Byte-Paired Encoding")]),t._v(", except that it merge small tokens which increase the likelihood on the training data the most.")]),t._v(" "),s("p",[t._v("In the paper, it is confusing:")]),t._v(" "),s("p",[t._v('they need to "maximize the likelihood" of each pair, by building a new Language Model (LM) each step - they don\'t say what constitutes a LM')]),t._v(" "),s("p",[t._v("In "),s("a",{attrs:{href:"https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece",target:"_blank",rel:"noopener noreferrer"}},[t._v("huggingface"),s("OutboundLink")],1),t._v(", it is interpreted as:")]),t._v(" "),s("p",[t._v('So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by its second symbol is the greatest among all symbol pairs. E.g. "u", followed by "g" would have only been merged if the probability of "ug" divided by "u", "g" would have been greater than for any other symbol pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it loses by merging two symbols to ensure it’s worth it.')]),t._v(" "),s("p",[t._v("In "),s("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/445686202",target:"_blank",rel:"noopener noreferrer"}},[t._v("zhihu"),s("OutboundLink")],1),t._v(", it is interpreted as:")]),t._v(" "),s("p",[t._v("合并比如决策树，在某个节点拆分前，会考虑到拆分前和拆分后的信息增益，比如我们选择了x这个特征进行拆分，那么在拆分前，其信息熵为 "),s("eq",[s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("mo",[t._v("−")]),s("mi",[t._v("l")]),s("mi",[t._v("o")]),s("mi",[t._v("g")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("p")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("y")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("-log(p(y)")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),s("span",{staticClass:"mord"},[t._v("−")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),s("span",{staticClass:"mord mathdefault"},[t._v("o")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("g")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault"},[t._v("p")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),s("span",{staticClass:"mclose"},[t._v(")")])])])])]),t._v(", 拆分后为: "),s("eq",[s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("mo",[t._v("−")]),s("mi",[t._v("l")]),s("mi",[t._v("o")]),s("mi",[t._v("g")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("p")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("y")]),s("mn",[t._v("1")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mo",[t._v("−")]),s("mi",[t._v("l")]),s("mi",[t._v("o")]),s("mi",[t._v("g")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("p")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("y")]),s("mn",[t._v("2")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("-log(p(y1)) - log(p(y2))")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),s("span",{staticClass:"mord"},[t._v("−")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),s("span",{staticClass:"mord mathdefault"},[t._v("o")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("g")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault"},[t._v("p")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),s("span",{staticClass:"mord"},[t._v("1")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),s("span",{staticClass:"mbin"},[t._v("−")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),s("span",{staticClass:"mord mathdefault"},[t._v("o")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("g")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault"},[t._v("p")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),s("span",{staticClass:"mord"},[t._v("2")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mclose"},[t._v(")")])])])])]),t._v(" , 也就是说，拆分前后的信息增益为："),s("eq",[s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("mo",[t._v("−")]),s("mi",[t._v("l")]),s("mi",[t._v("o")]),s("mi",[t._v("g")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("p")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("y")]),s("mn",[t._v("1")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mo",[t._v("−")]),s("mi",[t._v("l")]),s("mi",[t._v("o")]),s("mi",[t._v("g")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("p")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("y")]),s("mn",[t._v("2")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mo",[t._v("+")]),s("mi",[t._v("l")]),s("mi",[t._v("o")]),s("mi",[t._v("g")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("p")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("y")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("-log(p(y1)) - log(p(y2)) + log(p(y))")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),s("span",{staticClass:"mord"},[t._v("−")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),s("span",{staticClass:"mord mathdefault"},[t._v("o")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("g")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault"},[t._v("p")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),s("span",{staticClass:"mord"},[t._v("1")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),s("span",{staticClass:"mbin"},[t._v("−")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),s("span",{staticClass:"mord mathdefault"},[t._v("o")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("g")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault"},[t._v("p")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),s("span",{staticClass:"mord"},[t._v("2")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),s("span",{staticClass:"mbin"},[t._v("+")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),s("span",{staticClass:"mord mathdefault"},[t._v("o")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("g")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault"},[t._v("p")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mclose"},[t._v(")")])])])])]),t._v("， 整合后的结果是： "),s("eq",[s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("mi",[t._v("log")]),s("mo",[t._v("⁡")]),s("mrow",[s("mo",{attrs:{fence:"true"}},[t._v("(")]),s("mfrac",[s("mrow",[s("mi",[t._v("p")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("y")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),s("mrow",[s("mi",[t._v("p")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("y")]),s("mn",[t._v("1")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mi",[t._v("p")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("y")]),s("mn",[t._v("2")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1)],1),s("mo",{attrs:{fence:"true"}},[t._v(")")])],1)],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\log \\left(\\frac{p(y)}{p(y 1) p(y 2)}\\right)")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1.80002em","vertical-align":"-0.65002em"}}),s("span",{staticClass:"mop"},[t._v("lo"),s("span",{staticStyle:{"margin-right":"0.01389em"}},[t._v("g")])]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),s("span",{staticClass:"minner"},[s("span",{staticClass:"mopen delimcenter",staticStyle:{top:"0em"}},[s("span",{staticClass:"delimsizing size2"},[t._v("(")])]),s("span",{staticClass:"mord"},[s("span",{staticClass:"mopen nulldelimiter"}),s("span",{staticClass:"mfrac"},[s("span",{staticClass:"vlist-t vlist-t2"},[s("span",{staticClass:"vlist-r"},[s("span",{staticClass:"vlist",staticStyle:{height:"1.01em"}},[s("span",{staticStyle:{top:"-2.655em"}},[s("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),s("span",{staticClass:"sizing reset-size6 size3 mtight"},[s("span",{staticClass:"mord mtight"},[s("span",{staticClass:"mord mathdefault mtight"},[t._v("p")]),s("span",{staticClass:"mopen mtight"},[t._v("(")]),s("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),s("span",{staticClass:"mord mtight"},[t._v("1")]),s("span",{staticClass:"mclose mtight"},[t._v(")")]),s("span",{staticClass:"mord mathdefault mtight"},[t._v("p")]),s("span",{staticClass:"mopen mtight"},[t._v("(")]),s("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),s("span",{staticClass:"mord mtight"},[t._v("2")]),s("span",{staticClass:"mclose mtight"},[t._v(")")])])])]),s("span",{staticStyle:{top:"-3.23em"}},[s("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),s("span",{staticClass:"frac-line",staticStyle:{"border-bottom-width":"0.04em"}})]),s("span",{staticStyle:{top:"-3.485em"}},[s("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),s("span",{staticClass:"sizing reset-size6 size3 mtight"},[s("span",{staticClass:"mord mtight"},[s("span",{staticClass:"mord mathdefault mtight"},[t._v("p")]),s("span",{staticClass:"mopen mtight"},[t._v("(")]),s("span",{staticClass:"mord mathdefault mtight",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),s("span",{staticClass:"mclose mtight"},[t._v(")")])])])])]),s("span",{staticClass:"vlist-s"},[t._v("​")])]),s("span",{staticClass:"vlist-r"},[s("span",{staticClass:"vlist",staticStyle:{height:"0.52em"}},[s("span")])])])]),s("span",{staticClass:"mclose nulldelimiter"})]),s("span",{staticClass:"mclose delimcenter",staticStyle:{top:"0em"}},[s("span",{staticClass:"delimsizing size2"},[t._v(")")])])])])])])]),t._v(', 也就是说"e"和"s"的合并考虑的是概率上的效果提升，而不是单纯的频次')],1),t._v(" "),s("h4",{attrs:{id:"top-down"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#top-down"}},[t._v("#")]),t._v(" Top down")]),t._v(" "),s("p",[t._v("Used by bert. Starting with words and breaking them down into smaller components until they hit the frequency threshold, or can't be broken down further. For Japanese, Chinese and Korean this top-down approach doesn't work since there are no explicit word units to start with.")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://www.tensorflow.org/text/guide/subwords_tokenizer#choosing_the_vocabulary",target:"_blank",rel:"noopener noreferrer"}},[t._v("Details in tensorflow text library"),s("OutboundLink")],1)]),t._v(" "),s("h2",{attrs:{id:"evaluation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#evaluation"}},[t._v("#")]),t._v(" Evaluation")]),t._v(" "),s("h3",{attrs:{id:"similarity"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#similarity"}},[t._v("#")]),t._v(" Similarity")]),t._v(" "),s("p",[t._v("way 1 "),s("a",{attrs:{href:"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("sklearn.feature_extraction.text.CountVectorizer")]),s("OutboundLink")],1),t._v(": tokenize corpus first, and get a count matrix for prediction and ground truth respectively which can be used to compute the matrix similarity score using cosine later")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature_extraction"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" CountVectorizer\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scipy\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# supports both char and word")]),t._v("\nvec "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CountVectorizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ngram_range"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" analyzer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'char'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nvec"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sentence 1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sentence 2'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nm1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vec"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("prediction"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("todense"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nm2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vec"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("todense"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("score_calc")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    score "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scipy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spatial"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cosine"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    score "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isna"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" score\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" score\n\ntmp_df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" m2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nscores "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tmp_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" score_calc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("way 2 Levenshtein distance (number of additions, substitutions, etc.)")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" fuzzywuzzy "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" fuzz\n\nStr1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Apple Inc."')]),t._v("\nStr2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"apple Inc"')]),t._v("\nRatio "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fuzz"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ratio"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Str1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lower"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("Str2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lower"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("references")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://www.datacamp.com/community/tutorials/fuzzy-string-python",target:"_blank",rel:"noopener noreferrer"}},[t._v("How to use fuzzywuzzy"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fuzzywuzzy explanation"),s("OutboundLink")],1)])])])])}),[],!1,null,null,null);a.default=n.exports}}]);